% correlation.tex
Given two random variables $X$ and $Y$, the Pearson correlation coefficient (PCC) is defined as 
\begin{equation}
  \rho_{XY} = \frac{C_{XY}}{\sigma_X\sigma_Y},
  \label{eq:pearson}
\end{equation}
where $C_{XY}$, $\sigma_X$, and $\sigma_Y$ denote the covariance of $X$ and $Y$, the standard deviation of $X$, and the standard deviation of $Y$, respectively.

In turn, the covariance $C_{XY}$ is given by
\begin{equation}
  C_{XY} = E\{(X-\eta_X)(Y - \eta_Y)\},
  \label{eq:cov}
\end{equation}
where $\eta_X$ and $\eta_Y$ represent the mean of $X$ and $Y$, respectively; and $E\{\cdot\}$ is the expectation operator.

When applied to $n$ paired samples $(x_1,y_1),\cdots,(x_n,y_n)$ equation~\ref{eq:pearson} turns into (sample PCC)
\begin{equation}
  r_{xy} = \frac{\sum_{i=1}^n(x_i-\bar x)(y_i - \bar y)}{\sqrt{\sum_{i=1}^n(x_i-\bar x)^2}\sqrt{\sum_{i=1}^n(y_i-\bar y)^2}}
\end{equation}
where $\bar x$ corresponds to the sample mean of $x_i$ ($\bar y$ is similarly defined) as follows.
\begin{equation}
  \bar x = \frac{1}{n}\sum_{i=1}^n x_i
  \label{eq:xsamplemean}
\end{equation}

Essentially, the PCC is a measure of statistical similarity between two random variables (or samples). In the area of visual attention modeling, we use the PCC to assess resemblance between a saliency map prediction (model outup) and eye-tracking data (ground-truth).
% features and limitations

The PCC ranges from -1 to 1. When the magnitude of the PCC is close to zero, it indicates the random variables are weakly correlated. As the magnitude of the PCC approaches 1, the random variables are increasingly more correlated (directly or inversely correlated, depending on whether the PCC goes towards 1 or -1, respectively).

Specifically, the PCC measures of the linear relationship between two random variables. Thus, for $\alpha \in \Re$ and $\beta \in \Re$, if 
\begin{equation}
	y = \alpha x + \beta,
	\label{eq:afim}
\end{equation}
then the PCC yields 1 or -1, depending on $\alpha$ is positive or negative, respectively.

Monotonicity in general (beyond linearity) are under-evaluated by the PCC. In this case, other metrics should be used, for instance, Spearman correlation coefficient. 
%(are there actual gains in considering Spearman rank correlation coefficient in this case?)

Figures \ref{fig:pcc01} -- \ref{fig:pcc04} demonstrate the concept.
\begin{figure}
	\includegraphics[width=\textwidth]{figs/pcc01}
	\caption{Scatter plot between random samples $x$ and $y$ (high direct correlation). In this figure, $y = 2x+1+\eta$, where $\eta$ is a normal random variable. Measured $r_{xy}=0.9146$}
	\label{fig:pcc01}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{figs/pcc02}
	\caption{Scatter plot between random samples $x$ and $y$ (high inverse correlation). In this figure, $y = -2x-1+\eta$, where $\eta$ is a normal random variable. Measured $r_{xy}=-0.9161$}
	\label{fig:pcc02}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{figs/pcc03}
	\caption{Scatter plot between random samples $x$ and $y$ (low correlation). Measured $r_{xy}=0.1120$}
	\label{fig:pcc03}
\end{figure}

\begin{figure}
	\includegraphics[width=\textwidth]{figs/pcc04}
	\caption{Scatter plot between random samples $x$ and $y$. In this figure, $y = 1-\exp{-x}+\eta$, where $\eta$ is a normal random variable. Measured $r_{xy}=0.8306$}
	\label{fig:pcc04}
\end{figure}

For signals with higher dimensions, we can calculate the PCC by forming stacked vectors from matrices or tensors.

% 1D
% *** example with CC of 1
% *** example with CC of -1
% *** example with CC near zero (really uncorrelated random variables)
% *** example with CC of monotonically related random variables (perhaps exponential relationship)
%
% 2D
% GBVS x fixations; Itti&Koch x fixations (need to get some eye-gaze data)
%
% Inference: for each image, we can calculate the CC against a ground-truth (several observers contributing). The image CC can be considered as a sample of the truth CC between the model and the reference. By taking many samples, we can have a better estimation of the true CC. Confidence intervals can be estimated too. Attention to Gaussianity constraints.
